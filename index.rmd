---
title: "Classsifying physical activity from smartphone data with Keras"
description: |
  Using Keras to train a convolutional neural network to classify physical activity. The dataset was built from the recordings of 30 subjects performing basic activities and postural transitions while carrying a waist-mounted smartphone with embedded inertial sensors.
author:
  - name: Nick Strayer 
    url: http://nickstrayer.me
    affiliation: Vanderbilt University
    affiliation_url: https://www.vanderbilt.edu/biostatistics-graduate/
date: "`r Sys.Date()`"
creative_commons: CC BY
repository_url: https://github.com/nstrayer/activity_detection_post
output: 
  radix::radix_article:
    self_contained: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

In this post we'll describe how to use smartphone accelerometer and gyroscope data to predict the physical activities of the individuals carrying the phones. The data used in this post comes from the [Smartphone-Based Recognition of Human Activities and Postural Transitions Data Set](http://archive.ics.uci.edu/ml/datasets/Smartphone-Based+Recognition+of+Human+Activities+and+Postural+Transitions) distributed by the University of California, Irvine. Thirty individuals were tasked with performing various basic activities with an attached smartphone recording movement using an accelerometer and gyroscope.

Before we begin, let's load the various libraries that we'll use in the analysis:

```{r}
library(keras)     # Neural Networks
library(tidyverse) # Data cleaning / Visualization
library(knitr)     # Table printing
library(rmarkdown) # Misc. output utitlies 
library(ggridges)  # Visualization
```


## Activities dataset

The data used in this post come from the [Smartphone-Based Recognition of Human Activities and Postural Transitions Data Set](http://archive.ics.uci.edu/ml/datasets/Smartphone-Based+Recognition+of+Human+Activities+and+Postural+Transitions) distributed by the University of California, Irvine. 

<aside>
Throughout this post, `data/` is the directory created by downloading and unziping this dataset.
</aside>


When downloaded from the link above, the data contains two different 'parts.' One that has been pre-processed using various feature extraction techniques such as fast-fourier transform, and another `RawData` section that simply supplies the raw X,Y,Z directions of an accelerometer and gyroscope. None of the standard noise filtering or feature extraction used in accelerometer data has been applied. This is the data set we will use. 

The motivation for working with the raw data in this post is to aid the transition of the code/concepts to time series data in less well-characterized domains. While a more accurate model could be made by utlizing the filtered/cleaned data provided, the filtering and transformation can vary greatly from task to task; requiring lots of manual effort and domain knowledge. One of the beautiful things about deep learning is the feature extraction is learned from the data, not outside knowledge. 

### Activity labels

The data has integer encodings for the activities which, while not important to the model itself, are helpful for use to see. Let's load them first. 

```{r}
activityLabels <- read.table("data/activity_labels.txt", 
                             col.names = c("number", "label")) 

activityLabels %>% kable(align = c("c", "l"))
```

Next, we load in the labels key for the `RawData`. This file is a list of all of the observations, or individual activity recordings, contained in the data set. The key for the columns is taken from the data `README.txt`. 

```
Column 1: experiment number ID, 
Column 2: user number ID, 
Column 3: activity number ID 
Column 4: Label start point 
Column 5: Label end point 
```

The start and end points are in number of signal log samples (recorded at 50hz).

Let's take a look at the first 50 rows:

```{r, layout="l-body-outset"}
labels <- read.table(
  "data/RawData/labels.txt",
  col.names = c("experiment", "userId", "activity", "startPos", "endPos")
)

labels %>% 
  head(50) %>% 
  paged_table()
```

### File names

Next, let's look at the actual files of the user data provided to us in `RawData/`

```{r}
dataFiles <- list.files("data/RawData")
dataFiles %>% head()
```

There is a three-part file naming scheme. The first part is the type of data the file contains: either `acc` for accelerometer or `gyro` for gyroscope. Next is the experiment number, and last is the user Id for the recording. Let's load these into a dataframe for ease of use later. 

```{r}
fileInfo <- data_frame(
  filePath = dataFiles
) %>%
  filter(filePath != "labels.txt") %>% 
  separate(filePath, sep = '_', 
           into = c("type", "experiment", "userId"), 
           remove = FALSE) %>% 
  mutate(
    experiment = str_remove(experiment, "exp"),
    userId = str_remove_all(userId, "user|\\.txt")
  ) %>% 
  spread(type, filePath)

fileInfo %>% head() %>% kable()
```

### Reading and gathering data

Before we can do anything with the data provided we need to get it into a model-friendly format. This means we want to have a list of observations, their class (or activity label), and the data corresponding to the recording. 

To obtain this we will scan through each of the recording files present in `dataFiles`, look up what observations are contained in the recording, extract those recordings and return everything to an easy to model with dataframe. 

```{r, eval = FALSE}
# Read contents of single file to a dataframe with accelerometer and gyro data.
readInData <- function(experiment, userId){
  genFilePath = function(type) {
    paste0("data/RawData/", type, "_exp",experiment, "_user", userId, ".txt")
  }  
  
  bind_cols(
    read.table(genFilePath("acc"), col.names = c("a_x", "a_y", "a_z")),
    read.table(genFilePath("gyro"), col.names = c("g_x", "g_y", "g_z"))
  )
}

# Function to read a given file and get the observations contained along
# with their classes.

loadFileData <- function(curExperiment, curUserId) {
  
  # load sensor data from file into dataframe
  allData <- readInData(curExperiment, curUserId)

  extractObservation <- function(startPos, endPos){
    allData[startPos:endPos,]
  }
  
  # get observation locations in this file from labels dataframe
  dataLabels <- labels %>% 
    filter(userId == as.integer(curUserId), 
           experiment == as.integer(curExperiment))
  

  # extract observations as dataframes and save as a column in dataframe.
  dataLabels %>% 
    mutate(
      data = map2(startPos, endPos, extractObservation)
    ) %>% 
    select(-startPos, -endPos)
}

# scan through all experiment and userId combos and gather data into a dataframe. 
allObservations <- map2_df(fileInfo$experiment, fileInfo$userId, loadFileData) %>% 
  right_join(activityLabels, by = c("activity" = "number")) %>% 
  rename(activityName = label)

# cache work. 
write_rds(allObservations, "allObservations.rds")
allObservations %>% dim()
```

```{r, echo = FALSE}
allObservations <- read_rds("allObservations.rds")
allObservations %>% dim()
```

## Exploring the data

Now that we have all the data loaded along with the `experiment`, `userId`, and `activity` labels, we can explore the data set. 

### Length of recordings

Let's first look at the length of the recordings by activity. 

```{r, layout="l-body-outset"}
allObservations %>% 
  mutate(recording_length = map_int(data,nrow)) %>% 
  ggplot(aes(x = recording_length, y = activityName)) +
  geom_density_ridges()
```

The fact there is such a difference in length of recording between the different activity types requires us to be a bit careful with how we proceed. If we train the model on every class at once we are going to have to pad all the observations to the length of the longest, which would leave a large majority of the observations with a huge proportion of their data being just padding-zeros. Because of this, we will fit our model to just the largest 'group' of observations length activities, these include `STAND_TO_SIT`, `STAND_TO_LIE`, `SIT_TO_STAND`, `SIT_TO_LIE`, `LIE_TO_STAND`, and `LIE_TO_SIT`. 


<aside>
It is notable that the activities that we have selected here are all 'transitions.' So in a way we are creating a change-point detection algorithm. 
</aside>


An interesting future direction would be attempting to use another architecture such as an RNN that can handle variable length inputs and training it on all the data. However, you would run the risk of the model learning simply that if the observation is long it is most likely one of the four longest classes which would not generalize to a scenario where you were running this model on a real-time-stream of data. 

### Filtering activities

Based on our work from above, let's subset the data to just be of the activities of interest. 

```{r, layout="l-body-outset"}
desiredActivities <- c(
  "STAND_TO_SIT", "SIT_TO_STAND", "SIT_TO_LIE", 
  "LIE_TO_SIT", "STAND_TO_LIE", "LIE_TO_STAND"  
)

filteredObservations <- allObservations %>% 
  filter(activityName %in% desiredActivities) %>% 
  mutate(observationId = 1:n())

filteredObservations %>% paged_table()
```

So after our aggressive pruning of the data we will have a respectable amount of data left upon which our model can learn. 

### Training/testing split

Before we go any further into exploring the data for our model, in an attempt to be as fair as possible with our performance measures, we need to split the data into a train and test set. Since each user performed all activities just once (with the exception of one who only did 10 of the 12 activities) by splitting on `userId` we will ensure that our model sees new people exclusively when we test it. 

```{r}
# get all users
userIds <- allObservations$userId %>% unique()

# randomly choose 24 (80% of 30 individuals) for training
set.seed(42) # seed for reproducability
trainIds <- sample(userIds, size = 24)

# set the rest of the users to the testing set
testIds <- setdiff(userIds,trainIds)

# filter data. 
trainData <- filteredObservations %>% 
  filter(userId %in% trainIds)

testData <- filteredObservations %>% 
  filter(userId %in% testIds)
```


### Visualizing activities

Now that we have trimmed our data by removing activities and splitting off a test set, we can actually visualize the data for each class to see if there's any immediately discernible shape that our model may be able to pick up on. 

First let's unpack our data from its dataframe of one-row-per-observation to a tidy version of all the observations. 

```{r}
unpackedObs <- 1:nrow(trainData) %>% 
  map_df(function(rowNum){
    dataRow <- trainData[rowNum, ]
    dataRow$data[[1]] %>% 
      mutate(
        activityName = dataRow$activityName, 
        observationId = dataRow$observationId,
        time = 1:n() )
  }) %>% 
  gather(reading, value, -time, -activityName, -observationId) %>% 
  separate(reading, into = c("type", "direction"), sep = "_") %>% 
  mutate(type = ifelse(type == "a", "acceleration", "gyro"))
```

Now we have an unpacked set of our observations, let's visualize them!

```{r,layout="l-screen-inset", fig.width=12, fig.height=4}
unpackedObs %>% 
  ggplot(aes(x = time, y = value, color = direction)) +
  geom_line(alpha = 0.2) +
  geom_smooth(se = FALSE, alpha = 0.7, size = 0.5) +
  facet_grid(type ~ activityName, scales = "free_y") +
  theme_minimal() +
  theme( axis.text.x = element_blank() )
```

So at least in the accelerometer data patterns definitely emerge. One would imagine that the model may have trouble with the differences between `LIE_TO_SIT` and `LIE_TO_STAND` as they have a similar profile on average. The same goes for `SIT_TO_STAND` and `STAND_TO_SIT`. 

## Preprocessing

Before we can train the neural network, we need to take a couple of steps to preprocess the data.

### Padding observations

First we will decide what length to pad (and truncate) our sequences to by finding what the 98th percentile length is. By not using the very longest observation length this will help us avoid extra-long outlier recordings messing up the padding. 

```{r}
padSize <- trainData$data %>% 
  map_int(nrow) %>% 
  quantile(p = 0.98) %>% 
  ceiling()
padSize
```
Now we simply need to convert our list of observations to matrices, then use the super handy [`pad_sequences()`](https://keras.rstudio.com/reference/pad_sequences.html) function in Keras to pad all observations and turn them into a 3D tensor for us. 

```{r}
convertToTensor <- . %>% 
  map(as.matrix) %>% 
  pad_sequences(maxlen = padSize)

trainObs <- trainData$data %>% convertToTensor()
testObs <- testData$data %>% convertToTensor()
  
dim(trainObs)
```

Wonderful, we now have our data in a nice neural-network-friendly format of a 3D tensor with dimensions `(<num obs>, <sequence length>, <channels>)`. 

<aside> If we were working with a video instead of sensor data, this would be a 4D Tensor. If we were using FMRI data, this could be a 5D tensor! </aside>

### One-hot encoding

There's one last thing we need to do before we can train our model, and that is turn our observation classes from integers into one-hot, or dummy encoded, vectors. Luckily, again Keras has supplied us with a very helpful function to do just this. 

```{r}
oneHotClasses <- . %>% 
  {. - 7} %>%        # bring integers down to 0-6 from 7-12
  to_categorical() # One-hot encode

trainY <- trainData$activity %>% oneHotClasses()
testY <- testData$activity %>% oneHotClasses()
```


## Modeling

### Architecture

Since we have temporally dense time-series data we will make use of 1D convolutional layers. With temporally-dense data, an RNN has to learn very long dependencies in order to pick up on patterns, CNNs can simply stack a few convolutional layers to build pattern representations of substantial length. Since we are also simply looking for a single classification of activity for each observation, we can just use pooling to 'summarize' the CNNs view of the data into a dense layer.

In addition to stacking two [`layer_conv_1d()`](https://keras.rstudio.com/reference/layer_conv_1d.html) layers, we will use batch norm and dropout ([the spatial variant](https://keras.rstudio.com/reference/layer_spatial_dropout_1d.html) on the convolutional layers and [standard](https://keras.rstudio.com/reference/layer_dropout.html) on the dense) to regularize the network. 

```{r, layout="l-body-outset", width=90}
input_shape <- dim(trainObs)[-1]
num_classes <- dim(trainY)[2]

filters <- 24     # number of convolutional filters to learn
kernel_size <- 8  # how many time-steps each conv layer sees.
dense_size <- 48  # size of our penultimate dense layer. 

# Initialize model
model <- keras_model_sequential()
model %>% 
  layer_conv_1d(
    filters = filters,
    kernel_size = kernel_size, 
    input_shape = input_shape,
    padding = "valid", 
    activation = "relu"
  ) %>%
  layer_batch_normalization() %>%
  layer_spatial_dropout_1d(0.15) %>% 
  layer_conv_1d(
    filters = filters/2,
    kernel_size = kernel_size,
    activation = "relu",
  ) %>%
  # Apply average pooling:
  layer_global_average_pooling_1d() %>% 
  layer_batch_normalization() %>%
  layer_dropout(0.2) %>% 
  layer_dense(
    dense_size,
    activation = "relu"
  ) %>% 
  layer_batch_normalization() %>%
  layer_dropout(0.25) %>% 
  layer_dense(
    num_classes, 
    activation = "softmax",
    name = "dense_output"
  ) 

summary(model)
```


### Training

Now we can train the model using our test and training data. Note that we use `callback_model_checkpoint()` to ensure that we save only the best variation of the model (desirable since at some point in training the model may begin to overfit or otherwise stop improving).


```{r, eval = FALSE}
# Compile model
model %>% compile(
  loss = "categorical_crossentropy",
  optimizer = "rmsprop",
  metrics = "accuracy"
)

trainHistory <- model %>%
  fit(
    x = trainObs, y = trainY,
    epochs = 350,
    validation_data = list(testObs, testY),
    callbacks = list(
      callback_model_checkpoint("best_model.h5", 
                                save_best_only = TRUE)
    )
  )
```


```{r, eval = FALSE}
# Compile model
model %>% compile(
  loss = "categorical_crossentropy",
  optimizer = "rmsprop",
  metrics = "accuracy"
)

# dataframe to get labels onto one-hot encoded prediction columns
oneHotToLabel <- activityLabels %>% 
  mutate(number = number - 7) %>% 
  filter(number >= 0) %>% 
  mutate(class = paste0("V",number + 1)) %>% 
  select(-number)

write_rds(NULL,'epoch_perf.rds')
getTestPerformance <- function(epoch = 1, logs = NULL){
  
  currentEpoch <- model %>% 
    predict(testObs) %>% 
    as_data_frame() %>% 
    mutate(obs = 1:n()) %>% 
    gather(class, prob, -obs) %>% 
    right_join(oneHotToLabel, by = "class") %>% 
    group_by(obs) %>% 
    summarise(
      highestProb = max(prob),
      predicted = label[prob == highestProb]
    ) %>% 
    mutate(
      truth = testData$activityName,
      correct = truth == predicted
    ) %>% 
    group_by(truth, predicted) %>% 
    summarise(count = n()) %>% 
    mutate(
      good = truth == predicted,
      epoch = epoch
    )
  
  bind_rows(
    read_rds('epoch_perf.rds'), 
    currentEpoch
  ) %>% 
    write_rds('epoch_perf.rds')
}


trainHistory <- model %>%
  fit(
    x = trainObs, y = trainY,
    epochs = 100,
    validation_data = list(testObs, testY),
    callbacks = list(
      callback_lambda(on_epoch_end = getTestPerformance)
    )
  )
```


```{r, eval=FALSE, echo=FALSE}
saveRDS(trainHistory, "trainHistory.rds") # cache
```

```{r, echo = FALSE}
trainHistory <- read_rds("trainHistory.rds")
```


```{r, echo = FALSE, layout = 'l-page', fig.width = 7, fig.height = 4}
historyDf <- trainHistory %>% as.data.frame() 
best_results <- historyDf %>% 
  group_by(metric) %>% 
  summarise(
    low = min(value),
    lowEpoch = first(epoch[value == low]),
    high = max(value),
    highEpoch = first(epoch[value == high])
  ) %>% 
  mutate( 
    best = ifelse(metric == "loss", low, high),
    bestEpoch = ifelse(metric == "loss", lowEpoch, highEpoch),
    ypos = ifelse(metric == "loss", 1.5, 0.4)
  ) 

historyDf %>% 
  ggplot(aes(x = epoch, y = value, color = data)) +
  geom_text(
    data = best_results, 
    aes(x = 175, y = ypos, label = paste("best:",round(best,3))),
    color = "black", fill = "black", hjust = 1, nudge_x = -2
  ) +
  geom_segment(
    data = best_results, 
    aes(x = 175, xend = bestEpoch, y = ypos, yend = best), 
    color = "black", fill = "black", alpha = 0.4
  ) +
  geom_line(size = 0.2) +
  geom_point(size = 0.4) +
  facet_grid(metric~., scales = "free_y") +
  theme_minimal()
```

The model is learning something! We get a respectable `r scales::percent(best_results$best[2])` accuracy on the validation data, not bad with six possible classes to choose from. Let's look into the validation performance a little deeper to see where the model is messing up. 

### Evaluation

Now that we have a trained model let's investigate the errors that it made on our testing data. We can load the best model from training based upon validation accuracy and then look at each observation, what the model predicted, how high a probability it assigned, and the true activity label. 

```{r, layout="l-body-outset"}
# dataframe to get labels onto one-hot encoded prediction columns
oneHotToLabel <- activityLabels %>% 
  mutate(number = number - 7) %>% 
  filter(number >= 0) %>% 
  mutate(class = paste0("V",number + 1)) %>% 
  select(-number)

# Load our best model checkpoint
bestModel <- load_model_hdf5("best_model.h5")

tidyPredictionProbs <- bestModel %>% 
  predict(testObs) %>% 
  as_data_frame() %>% 
  mutate(obs = 1:n()) %>% 
  gather(class, prob, -obs) %>% 
  right_join(oneHotToLabel, by = "class")

predictionPerformance <- tidyPredictionProbs %>% 
  group_by(obs) %>% 
  summarise(
    highestProb = max(prob),
    predicted = label[prob == highestProb]
  ) %>% 
  mutate(
    truth = testData$activityName,
    correct = truth == predicted
  ) 

predictionPerformance %>% paged_table()
```

First, let's look at how 'confident' the model was by if the prediction was correct or not. 

```{r, layout ="l-body-outset"}
predictionPerformance %>% 
  mutate(result = ifelse(correct, 'Correct', 'Incorrect')) %>% 
  ggplot(aes(highestProb)) +
  geom_histogram(binwidth = 0.01) +
  geom_rug(alpha = 0.5) +
  facet_grid(result~.) +
  ggtitle("Probabilities associated with prediction by correctness")
```

Reassuringly we see that the model was, on average, less confident about its classifications for the incorrect results than the correct ones. 

Let's see what activities the model had the hardest time with using a confusion matrix.

```{r, layout = 'l-page', fig.width = 8, fig.height = 5}
predictionPerformance %>% 
  group_by(truth, predicted) %>% 
  summarise(count = n()) %>% 
  mutate(good = truth == predicted) %>% 
  ggplot(aes(x = truth,  y = predicted)) +
  geom_point(aes(size = count, color = good)) +
  geom_text(aes(label = count), 
            hjust = 0, vjust = 0, 
            nudge_x = 0.1, nudge_y = 0.1) + 
  guides(color = FALSE, size = FALSE) +
  theme_minimal()
```

We see that, as the preliminary visualization suggested, the model had a bit of trouble with distinguishing between `LIE_TO_SIT` and `LIE_TO_STAND` classes, along with the `SIT_TO_LIE` and `STAND_TO_LIE`, which also have similar visual profiles. 

## Future directions

The most obvious future direction to take this analysis would be to attempt to make the model more general by working with more of the supplied activity types. Another interesting direction would be to not separate the recordings into distinct 'observations' but instead keep them as one streaming set of data, much like a real world deployment of a model would work, and see how well a model could classify streaming data and detect changes in activity. 




